{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@FollowerOfScriabin\n",
    "\n",
    "References:\n",
    "https://www.tensorflow.org/guide/datasets\n",
    "\n",
    "https://cyc1am3n.github.io/2018/09/13/how-to-use-dataset-in-tensorflow.html\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "\n",
    "\n",
    "https://hiseon.me/2018/04/15/tensorflow-dataset/\n",
    "\n",
    "https://locslab.github.io/Tensorflow-Dataset-API(2)/\n",
    "\n",
    "https://stackoverflow.com/questions/47091726/difference-between-tf-data-dataset-map-and-tf-data-dataset-apply\n",
    "\n",
    "https://stackoverflow.com/questions/45292517/how-do-i-use-the-group-by-window-function-in-tensorflow\n",
    "\n",
    "\n",
    "# How to process a data with tf.data.Dataset?\n",
    "### Explained by diverse examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static numpy data : one shot iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.make one shot iter cannot use placeholder as input. It can't parameterize.\n",
    "x = tf.placeholder(tf.float32,shape=[None,2])\n",
    "\n",
    "e = np.random.sample((3,6,2))\n",
    "data = tf.data.Dataset.from_tensor_slices(e)\n",
    "print(data.output_types, data.output_shapes)\n",
    "\n",
    "iterator = data.make_one_shot_iterator()\n",
    "features = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(features))\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            #print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic numpy data : initializable iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use initializable iterator to input placeholder\n",
    "#or it can use to process dict/namedtuple\n",
    "e = np.random.sample((10,2))\n",
    "x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "data = tf.data.Dataset.from_tensor_slices(x)\n",
    "\n",
    "iterator = data.make_initializable_iterator()\n",
    "\n",
    "#Note: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n",
    "#dataset = ...\n",
    "#iterator = dataset.make_initializable_iterator()\n",
    "# sess.run(iterator.initializer)\n",
    "\n",
    "features = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict = {x:e})\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(features))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = (np.random.sample((10,2)), np.random.sample((10,1)))\n",
    "test_x = (np.ones([1,2]).astype(np.int32), np.zeros([1,1]).astype(np.int32))\n",
    "epochs = 10\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features, labels = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={ x: train_x[0], y: train_x[1]})\n",
    "    for _ in range(epochs):\n",
    "        sess.run([features, labels])\n",
    "#     switch to test data\n",
    "    sess.run(iter.initializer, feed_dict={ x: test_x[0], y: test_x[1]})\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic numpy data : reinitializable iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitializable iterator to switch between Datasets\n",
    "train_x = tf.data.Dataset.from_tensor_slices((np.random.sample((10,2)), np.random.sample((10,1))))\n",
    "test_x = tf.data.Dataset.from_tensor_slices((np.random.sample((5,2)), np.random.sample((5,1))))\n",
    "epochs = 10\n",
    "iter = tf.data.Iterator.from_structure(train_x.output_types,train_x.output_shapes)\n",
    "features, labels = iter.get_next()\n",
    "\n",
    "train_init = iter.make_initializer(train_x)\n",
    "test_init = iter.make_initializer(test_x)\n",
    "with tf.Session() as sess:\n",
    "#     initialise iterator with train data\n",
    "    sess.run(train_init)\n",
    "    for _ in range(epochs):\n",
    "        print(sess.run([features, labels]))\n",
    "#     switch to test data\n",
    "    sess.run(test_init)\n",
    "    print(sess.run([features, labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = np.array([[1,2],[3],[4,5,6],[7]])\n",
    "def gen():\n",
    "    for element in sequence:\n",
    "        yield element\n",
    "dataset = tf.data.Dataset.from_generator(gen, output_types=(tf.int32),\n",
    "                                         output_shapes=(tf.TensorShape([None]))) # scalar : []\n",
    "iter = dataset.make_initializable_iterator()\n",
    "features = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer)\n",
    "    print(sess.run(features))\n",
    "    print(sess.run(features))\n",
    "    #print(sess.run(features))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NOTE: The current implementation of Dataset.from_generator() uses tf.py_func and inherits the same constraints. In particular, it requires the Dataset- and Iterator-related operations to be placed on a device in the same process as the Python program that called Dataset.from_generator(). The body of generator will not be serialized in a GraphDef, and you should not use this method if you need to serialize your model and restore it in a different environment.\n",
    "\n",
    "* NOTE: If generator depends on mutable global variables or other external state, be aware that the runtime may invoke generator multiple times (in order to support repeating the Dataset) and at any time between the call to Dataset.from_generator() and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in generator before calling Dataset.from_generator()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static dict, namedtuple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#namedtuple\n",
    "Sample = collections.namedtuple('sample_data', ['a','b']) # ['a','b'] ='a b'\n",
    "sample_data = Sample(\n",
    "    tf.random_uniform([4],maxval=5,dtype=tf.int32), tf.random_uniform([4, 100], \n",
    "                                              maxval=100, dtype=tf.int32))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(sample_data)\n",
    "print(dataset.output_types)     # ==> sample_data(a=tf.float32, b=tf.int32)\n",
    "print(dataset.output_shapes)    # ==> sample_data(a=TensorShape([]), b=TensorShape([Dimension(100)]))\n",
    "print(dataset.output_classes)\n",
    "print(dataset.output_types.a)   # ==> <dtype: 'float32'>\n",
    "print(dataset.output_types.b)   # ==> <dtype: 'int32'>\n",
    "print(dataset.output_shapes.a)  # ==> ()\n",
    "print(dataset.output_shapes.b)  # ==> (100, )\n",
    "\n",
    "\n",
    "# dictionary\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    {\n",
    "        'a': tf.random_uniform([4]),\n",
    "        'b': tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)\n",
    "    }\n",
    ")\n",
    "print(dataset.output_types)     # ==> {'a' : tf.float32, 'b' : tf.int32}\n",
    "print(dataset.output_shapes)    # ==> {'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n",
    "print(dataset.output_types['a'])    # ==> <dtype: 'float32'>\n",
    "print(dataset.output_types['b'])    # ==> <dtype: 'int32'>\n",
    "print(dataset.output_shapes['a'])   # ==> ()\n",
    "print(dataset.output_shapes['b'])   # ==> (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict =    {\n",
    "        'a': np.random.sample((10,1)),\n",
    "        'b': np.random.sample((10,4))\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "x, y = tf.placeholder(tf.float32, shape=[None,1]), tf.placeholder(tf.float32, shape=[None,4])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "features, labels = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict = {x:dict['a'], y:dict['b']})\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run([features,labels]))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "#Note: The returned iterator will be in an uninitialized state, and you must run the iterator.initializer operation before using it:\n",
    "#dataset = ...\n",
    "#iterator = dataset.make_initializable_iterator()\n",
    "# sess.run(iterator.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Transformation by groupbywindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def al(x,r):\n",
    "    print(x.shape,type(x))\n",
    "    print(r,type(r),type(r.batch(3)))\n",
    "    return r.batch(10)\n",
    "\n",
    "e = np.arange(100).astype(np.int64)\n",
    "data = tf.data.Dataset.from_tensor_slices(e) # = tf.data.Dataset.range(100)\n",
    "dataset = data.apply(tf.contrib.data.group_by_window(key_func=lambda x: x% 3, reduce_func= al,window_size=100))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "features = iterator.get_next()\n",
    "sess = tf.Session()\n",
    "sess.run(features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Transformation by map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices(e)\n",
    "dataset = data.map(lambda x: x*2)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "features = iterator.get_next()\n",
    "sess = tf.Session()\n",
    "print(sess.run(features))\n",
    "print(sess.run(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The difference is that map will execute one function on every element of the Dataset separately, whereas apply will execute one function on the whole Dataset at once (such as group_by_window given as example in the documentation).\n",
    "\n",
    "* The argument of apply is a function that takes a Dataset and returns a Dataset when the argument of map is a function that takes one element and returns one transformed element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset and dataset to be concatenated should have same\n",
    "# nested structures and output types.\n",
    "# c = { (8, 9), (10, 11), (12, 13) }\n",
    "# d = { 14.0, 15.0, 16.0 }\n",
    "# a.concatenate(c) and a.concatenate(d) would result in error.\n",
    "\n",
    "e1 = np.arange(100).astype(np.int64)\n",
    "data1 = tf.data.Dataset.from_tensor_slices(e1)\n",
    "e2 = np.arange(100).astype(np.int64)\n",
    "data2 = tf.data.Dataset.from_tensor_slices(e2)\n",
    "\n",
    "data1.concatenate(data2) # cocatenate two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shard (for distributed computing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md\n",
    "'''\n",
    "e = np.arange(100).astype(np.int64)\n",
    "data = tf.data.Dataset.from_tensor_slices(e)\n",
    "data = data.shard(FLAGS.num_workers, FLAGS.worker_index)\n",
    "iter = data.make_one_shot_iterator()\n",
    "features = iter.get_next()\n",
    "with tf.Session as sess:\n",
    "    for i in range(input):\n",
    "        print(sess.run(features))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static\n",
    "\n",
    "def filter_func(x):\n",
    "    print(x.shape)\n",
    "    r = tf.equal(x % 4,0)\n",
    "    return r\n",
    "    \n",
    "e = np.arange(100).astype(np.int64)\n",
    "data = tf.data.Dataset.from_tensor_slices(e)\n",
    "print(data)\n",
    "data_filtered = data.filter(predicate=filter_func)\n",
    "iter = data_filtered.make_one_shot_iterator()\n",
    "features = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(features))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic\n",
    "def filter_func(x):\n",
    "    r = tf.greater(x,5)\n",
    "    return r\n",
    "\n",
    "shape = (10,3)\n",
    "e = np.random.sample(shape) * 10\n",
    "e = e.flatten()\n",
    "\n",
    "x= tf.placeholder(tf.float32, shape=[None])\n",
    "data = tf.data.Dataset.from_tensor_slices(x)\n",
    "data_filtered = data.filter(filter_func)\n",
    "iter = data_filtered.make_initializable_iterator()\n",
    "features = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iter.initializer, feed_dict={x:e})\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(features))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e= np.random.sample((10,3))\n",
    "e = tf.data.Dataset.from_tensor_slices(e)\n",
    "print(e.output_shapes,e.output_types)\n",
    "data = e.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "print(data.output_shapes,data.output_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e = tf.data.Dataset.from_tensor_slices(np.random.sample((10,3)) * 10)\n",
    "data_flatten = e.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "\n",
    "def filter_func(t):\n",
    "    r = tf.greater(t,5)\n",
    "    print(r.shape)\n",
    "    return(r)\n",
    "    \n",
    "data_filtered = data_flatten.filter(predicate=filter_func)\n",
    "iter = data_filtered.make_one_shot_iterator()\n",
    "features = iter.get_next()\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(features))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repeat, batch, shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.make one shot iter cannot use placeholder as input. It can't parameterize.\n",
    "x = tf.placeholder(tf.float32,shape=[None,2])\n",
    "\n",
    "e = np.random.sample((20,2))\n",
    "data = tf.data.Dataset.from_tensor_slices(e)\n",
    "data = data.shuffle(30).repeat(5).batch(10) #shuffle data and repeat 5 times, and extract batch by 10\n",
    "\n",
    "iterator = data.make_one_shot_iterator()\n",
    "features = iterator.get_next()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    while True:\n",
    "        try:\n",
    "            print(sess.run(features))\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            #print(e)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
